{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Scan Simulation/Rendering\n",
    "\n",
    "## Initialization and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/sebastian/Projects/SRB/Package/repo/scanner-sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'cam_width': 100, 'cam_height': 100, 'cam_fov_y': 0.38946283611979604, 'cam_pixelAspect': 0.9992771751028543, 'cam_focus': 0.8103669534556511, 'cam_aperture': 0.002, 'cam_diffLimit': 0.01028416270425193, 'proj_width': 1920, 'proj_height': 1080, 'proj_scaleX': 2805.329833984375, 'proj_scaleY': 2805.11572265625, 'proj_offsetX': 958.3337814131664, 'proj_offsetY': 1095.1464166282676, 'proj_intensity': 10, 'proj_gapSize': 0, 'proj_focus': 0.4902099376513703, 'proj_aperture': 0.0035951384447263915, 'proj_diffLimit': 0.0057211497483984144}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'projector_focus.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-06d12cff3dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Config:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"projector_focus.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mensure_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/SRB/Package/repo/scanner-sim/simulator/rendering/rendering.py\u001b[0m in \u001b[0;36mload_template\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'projector_focus.xml'"
     ]
    }
   ],
   "source": [
    "from simulator.rendering import *\n",
    "\n",
    "data_path = \"../data/test\"\n",
    "config, cam_geom = {}, load_calibration(\"../scanner/calibration/camera/camera_geometry.json\")\n",
    "# Render small crop (200 pixels high) only for optimal performance\n",
    "cam_geom[\"image_width, pixels\"] = 100\n",
    "cam_geom[\"image_height, pixels\"] = 100\n",
    "\n",
    "configure_camera_geometry(config, cam_geom)\n",
    "configure_camera_focus(config, \"../scanner/calibration/camera/camera_focus.json\")\n",
    "\n",
    "configure_projector_geometry(config, \"../scanner/calibration/projector/projector_geometry.json\")\n",
    "configure_projector_focus(config, \"../scanner/calibration/projector/projector_focus.json\")\n",
    "config[\"proj_diffLimit\"] = config[\"cam_diffLimit\"] * config[\"cam_aperture\"] / config[\"proj_aperture\"]\n",
    "# config[\"proj_diffLimit\"] = 0\n",
    "# config[\"proj_aperture\"] = 0.001\n",
    "\n",
    "\n",
    "print(\"Config:\", config)\n",
    "\n",
    "header, body = load_template(\"projector_focus.xml\")\n",
    "ensure_exists(data_path + \"/\")\n",
    "\n",
    "h, w = 1080, 1920\n",
    "pattern = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "# pattern[:, :, 0] = np.random.randint(255, size=(h, w))\n",
    "pattern[h-1, w//2, :] = 255\n",
    "# pattern[:, w//2 - 10, :] = 255\n",
    "# pattern[:, w//2 - 8, :] = 255\n",
    "# pattern[h-10, :, :] = 255\n",
    "# pattern[h-8, :, :] = 255\n",
    "imageio.imwrite(data_path + \"/pattern.png\", pattern)\n",
    "# imageio.imwrite(data_path + \"/pattern.png\", pattern[::10, ::10, :])\n",
    "\n",
    "config[\"scale\"] = scale\n",
    "config[\"offset\"] = config[\"cam_focus\"] - config[\"proj_focus\"]\n",
    "config[\"proj_offsetX\"], config[\"proj_offsetY\"] = w/2, h\n",
    "\n",
    "for dist_cm in range(*range_cm):\n",
    "    # Geometry cube is 2*scale meters in size\n",
    "    config[\"dist\"] = config[\"scale\"] + dist_cm / 100.\n",
    "    generate_scene(header, body, config, data_path + \"/dist_%d.xml\" % dist_cm)\n",
    "\n",
    "source(mitsuba_path + \"/setpath.sh\")\n",
    "render_scenes(data_path + \"/dist_*.xml\", verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.chdir(\"/sls/scanner-sim\")\n",
    "from rendering import render_object, generate_render_parameters\n",
    "\n",
    "def save_config_and_run_scan(config_name, config):\n",
    "    # Store rendering parameters for reference\n",
    "    with open(config_name, \"w\") as fi:\n",
    "        json.dump(config, fi, indent=2, sort_keys=True)  \n",
    "    \n",
    "    # Run render job which takes roughly 10 minutes to finish all images of the scan\n",
    "    render_object(config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering an object with default Gray code patterns\n",
    "\n",
    "As a first step, we need to define the parameters for the simulation/renderer. This is done with the *generate_render_parameters()* function which supports many different settings. In this example, let's choose the shapes object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = generate_render_parameters(obj_path=\"objects/machined/shapes.obj\", # Path to the object (relative to the data folder)\n",
    "                                  result_path=\"results/shapes\", # Path where the results are stored\n",
    "                                  typ=\"shapes\", # Predefined settings for the shapes object to match the real scanner\n",
    "                                  size=\"small\", # Resolution of the renderings (large=original camera size, medium=original/2, small=original/4)\n",
    "                                  samples=64, # Samples per pixel that are used for the rendering\n",
    "                                  patterns=\"patterns/gray\" # The patterns that are used for the scanning process\n",
    "                                 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a parameter dictionary with default settings (matching the physical scanner) which can be subsequently modified if needed. The parameter dictionary is then stored and the simulation/rendering is started with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_config_and_run_scan(\"data/configs/parameters_shapes.json\", pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rendering the images, let's look at some results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"data/results/shapes/img_016.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"data/results/shapes/img_040.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering an object with different light coding patterns\n",
    "\n",
    "In order to use different patterns, the path to the patterns and additional settings can be modified. The following example shows how to scan the pawn object with micro phase shifting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = generate_render_parameters(obj_path=\"objects/machined/pawn.obj\", # The pawn object\n",
    "                                  result_path=\"results/pawn\", \n",
    "                                  typ=\"pawn\",\n",
    "                                  size=\"small\",\n",
    "                                  samples=64,\n",
    "                                  cpu_count=8, # Render with only 8 cores this time\n",
    "                                  patterns=\"patterns/mps/32-08\", # Select micro phase shifting patterns\n",
    "                                 )\n",
    "\n",
    "pars[\"pattern_colored\"] = False # We don't need support for colored patterns this time\n",
    "pars[\"pattern_calibrate\"] = True # Predistort the patterns so that they match the physical projector\n",
    "pars[\"pattern_flip_ud\"] = True # Flip the patterns vertically so that they are projected with an upright projector\n",
    "\n",
    "save_config_and_run_scan(\"data/configs/parameters_pawn.json\", pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"data/results/pawn/img_000.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering a colored/textured object (one single scan)\n",
    "\n",
    "Textured objects can be scanned/rendered similarly. Currently, only objects in *obj* format are supported. In this example, scan the object from only one direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = generate_render_parameters(obj_path=\"objects/colored/vase/vase.obj\", \n",
    "                                  result_path=\"results/vase\", \n",
    "                                  typ=\"vase\", # Render with vase presets\n",
    "                                  size=\"small\",\n",
    "                                  samples=64,\n",
    "                                  cpu_count=8,\n",
    "                                  patterns=\"patterns/gray\"\n",
    "                                 )\n",
    "\n",
    "pars[\"pattern_colored\"] = True\n",
    "\n",
    "save_config_and_run_scan(\"data/configs/parameters_vase.json\", pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"data/results/vase/img_018.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering a colored/textured object (multiple scans, turntable)\n",
    "\n",
    "In this example, we scan the object from multiple directions to reproduce the full geometry. The object is rotated on a virtual turntable and all scans can be merged together in the reconstruction phase (see below). Note that this rendering process will take a while (~20 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = generate_render_parameters(obj_path=\"objects/colored/vase/vase.obj\", \n",
    "                                  result_path=\"results/vase_full\", \n",
    "                                  typ=\"vase\", # Render with vase presets\n",
    "                                  size=\"small\",\n",
    "                                  samples=32, # Let's reduce the samples for faster scanning\n",
    "                                  cpu_count=8,\n",
    "                                  patterns=\"patterns/gray\"\n",
    "                                 )\n",
    "\n",
    "pars[\"pattern_colored\"] = True\n",
    "pars[\"rot_type\"] = \"Turntable\" # Enable turntable rotation\n",
    "pars[\"rot_range\"] = [0, 360, 4] # Rotate between 0 and 360 degrees in 4 steps\n",
    "\n",
    "save_config_and_run_scan(\"data/configs/parameters_vase_full.json\", pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the 4 different views..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,15))\n",
    "for i in range(4):\n",
    "    img = plt.imread(\"data/results/vase_full/rot_%03i/img_000.png\"%i)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Decoding and Reconstruction\n",
    "\n",
    "Attention: This stage depends on the results that were produced in stage 1. If you haven't run the relevant cells above, some cells will not work.\n",
    "\n",
    "## Initialization and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#os.chdir(\"/sls/scanner-sim\")\n",
    "import meshplot as mp\n",
    "from decoding import decode_gray, decode_mps\n",
    "from reconstruction import reconstruct_single, merge_single\n",
    "from utils import load_projector_calibration, load_camera_calibration, numpinize, load_openexr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding of a single scan (shapes object with Gray code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/results/shapes\"\n",
    "\n",
    "# Decode Gray coded images\n",
    "idx_h, idx_v = decode_gray(data_path, group=True, plot=True, sim=True, threshold=130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of a single scan (shapes object with Gray code)\n",
    "\n",
    "In this example, we reconstruct the depth map and the colored point cloud from the correspondence indices that were created during the decoding stage. The indices from the decoding stage don't need to be passed into the function, they are passed as files with configured filenames. In addition, we need to load the projector and camera calibration for the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load projector and camera calibration\n",
    "pro_calib = load_projector_calibration(\"data/calibrations/projector_calibration_new.json\")[2]\n",
    "cam_calib = load_camera_calibration(\"data/calibrations/camera_calibration_s_new.json\")\n",
    "\n",
    "data_path = \"data/results/shapes\"\n",
    "\n",
    "points, colors, depth_map = reconstruct_single(data_path, cam_calib, pro_calib, plot=True, sim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reconstructed depthmap compare to the ground truth depth map? For this we load the ground truth depth map from the renderings and plot the difference and a histogram over the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, gt_depth_map = load_openexr(\"data/results/shapes/img_000.exr\", load_depth=True)\n",
    "difference = gt_depth_map - depth_map\n",
    "hist_difference = np.abs(difference.reshape(-1))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,7))\n",
    "axs[0].imshow(difference)\n",
    "axs[0].axis('off')\n",
    "axs[1].hist(difference.reshape(-1), bins=np.linspace(0.00001, 0.002, 40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us also visualize the reconstructed point cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(points, c=colors, shading={\"point_size\": 5.0});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding and reconstruction of multiple scans (vase object with Gray code)\n",
    "\n",
    "Similar to before, we can decode the multiple scans of the colored vase object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load projector and camera calibration\n",
    "pro_calib = load_projector_calibration(\"data/calibrations/projector_calibration_new.json\")[2]\n",
    "cam_calib = load_camera_calibration(\"data/calibrations/camera_calibration_s_new.json\")\n",
    "\n",
    "def decode_and_reconstruct(data_path):\n",
    "    decode_gray(data_path, group=True, sim=True, threshold=50)\n",
    "    reconstruct_single(data_path, cam_calib, pro_calib, sim=True)\n",
    "\n",
    "\n",
    "data_paths = glob.glob(\"data/results/vase_full/rot_*\")\n",
    "\n",
    "for data_path in tqdm(data_paths):\n",
    "    decode_and_reconstruct(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge all the decoded and reconstructed scans together by rotating them to the same coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the turntable calib\n",
    "stage_calib = numpinize(json.load(open(\"data/calibrations/stage_calibration_sim.json\", \"r\")))\n",
    "\n",
    "# Merge the single scans\n",
    "points, normals, colors = merge_single(\"data/results/vase_full\", \"/rot_%s/reconstructed/group_points.ply\"%\"%03i\",\n",
    "                                     stage_calib, title=\"vase.ply\", max_range=4, sim=True)\n",
    "    \n",
    "# Plot the reconstructed point cloud\n",
    "mp.plot(points, c=colors, shading={\"point_size\": 5.0});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding of different patterns (pawn with micro phase shifting)\n",
    "\n",
    "It is possible to use different patterns for the scanning proces. This can be achieved by adding the patterns as images and a decoding stage, which reads in the rendered results and produces the correspondence map. For the three decoding strategies listed in the paper, we supply an implementation with our software. The following example shows the decoding for micro phase shifting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_h = decode_mps(\"data/results/pawn\", \"data/patterns/mps/32-08/\", cam=[1616, 1213])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(ic_h)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
